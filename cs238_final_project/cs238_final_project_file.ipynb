{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [list(range(3)) for _ in range(9)]\n",
    "all_states = list(set(product(*combinations)))\n",
    "#print(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_win(player, board):\n",
    "    if np.all(board[0,:] == player):\n",
    "        return True\n",
    "    elif np.all(board[1,:] == player): \n",
    "        return True\n",
    "    elif np.all(board[2,:] == player):\n",
    "        return True\n",
    "    elif np.all(board[:,0] == player):\n",
    "        return True\n",
    "    elif np.all(board[:,1] == player):\n",
    "        return True\n",
    "    elif np.all(board[:,2] == player):\n",
    "        return True\n",
    "    elif np.all(board.diagonal()==player):\n",
    "        return True\n",
    "    elif  np.all(np.fliplr(board).diagonal()==player):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             s  a  r     sp\n",
      "0         8802  4  0  14362\n",
      "1        14362  6  0   3065\n",
      "2         3065  0  0   3784\n",
      "3         3784  5  0  11396\n",
      "4         8802  8  1   6434\n",
      "...        ... .. ..    ...\n",
      "1037174   7321  2  1   5398\n",
      "1037175   5398  5  1   4483\n",
      "1037176   8802  6  1   4829\n",
      "1037177   4829  7  1  14087\n",
      "1037178  14087  8  1   1716\n",
      "\n",
      "[1037179 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#simulation with player O\n",
    "\n",
    "master_states_res = []\n",
    "master_actions_res = []\n",
    "master_rewards_res = []\n",
    "master_next_states_res = []\n",
    "\n",
    "current_player = \"O\"\n",
    "opponent_player = \"X\"\n",
    "\n",
    "for num_sims in range(300000):\n",
    "    empty_board = np.array(['-']*9).reshape(3,3)\n",
    "    board = empty_board\n",
    "\n",
    "    current_turn = \"X\"\n",
    "    x,y = np.where(board == '-')\n",
    "    possible_actions = [(x,y) for x,y in zip(x,y)]\n",
    "    avail_positions = [(x,y) for x,y in zip(x,y)]\n",
    "    #print(possible_actions)\n",
    "\n",
    "    empty_state_ind = all_states.index((0,0,0,0,0,0,0,0,0))\n",
    "    states_res = [empty_state_ind]\n",
    "    actions_res = []\n",
    "    next_states_res = []\n",
    "\n",
    "    while (not check_win(current_player, board) and not check_win(opponent_player, board) and (\"-\" in board)):\n",
    "        # choose random move\n",
    "        rand_pos = random.choice(avail_positions)\n",
    "        board[rand_pos[0]][rand_pos[1]] = current_turn\n",
    "        avail_positions.remove(rand_pos)\n",
    "\n",
    "        # if it's our current player's turn (not opponent's turn)\n",
    "        if current_turn == current_player: \n",
    "            # assign action to result\n",
    "            actions_res.append(possible_actions.index(rand_pos))\n",
    "\n",
    "            # get the tuple of the current state\n",
    "            cur_state = [0,0,0,0,0,0,0,0,0]\n",
    "            cur_flat_board = board.flatten()\n",
    "            for i in range(len(cur_flat_board)):\n",
    "                if cur_flat_board[i] == \"X\":\n",
    "                    cur_state[i] = 1\n",
    "                elif cur_flat_board[i] == \"O\":\n",
    "                    cur_state[i] = 2\n",
    "            cur_state = tuple(cur_state)\n",
    "\n",
    "            # assign states to results\n",
    "            cur_state_ind = all_states.index(cur_state)\n",
    "            states_res.append(cur_state_ind)\n",
    "            next_states_res.append(cur_state_ind)\n",
    "\n",
    "\n",
    "        # switch to other player for next turn\n",
    "        if current_turn == \"X\":\n",
    "            current_turn = \"O\"\n",
    "        else:\n",
    "            current_turn = \"X\"\n",
    "        #print(board)\n",
    "\n",
    "    # remove last element of states_res because we overshot\n",
    "    states_res.pop()\n",
    "\n",
    "    # assign appropriate reward       \n",
    "    if check_win(current_player, board):\n",
    "        reward = 1\n",
    "    elif check_win(opponent_player, board):\n",
    "        reward = -1\n",
    "    else:\n",
    "        reward = 0\n",
    "    rewards_res = [reward] * len(actions_res)\n",
    "    #print(rewards_res)\n",
    "\n",
    "    # add to master results\n",
    "    master_states_res.extend(states_res)\n",
    "    master_actions_res.extend(actions_res)\n",
    "    master_rewards_res.extend(rewards_res)\n",
    "    master_next_states_res.extend(next_states_res)\n",
    "\n",
    "\"\"\"\n",
    "print(master_states_res)\n",
    "print(master_actions_res)\n",
    "print(master_rewards_res)\n",
    "print(master_next_states_res)\n",
    "\"\"\"\n",
    "\n",
    "# make dataframe of master results\n",
    "\n",
    "results_df = pd.DataFrame({\"s\": master_states_res, \n",
    "                           \"a\": master_actions_res,\n",
    "                           \"r\": master_rewards_res,\n",
    "                           \"sp\": master_next_states_res})\n",
    "print(results_df)\n",
    "\n",
    "# write dataframe to csv file\n",
    "results_df.to_csv(\"simulation_results_player_x_combined\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"simulation_results_player_x_combined\"\n",
    "outfile_name = \"policy_results_player_x_combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = len(all_states)\n",
    "num_actions = len(possible_actions)\n",
    "gamma =  0.95\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_matrix(Q_matrix, s, a, r, sp, gamma, alpha):\n",
    "    Q_matrix[s-1, a-1] = Q_matrix[s-1, a-1] + alpha * (r + gamma*max(Q_matrix[sp-1,:]) - Q_matrix[s-1,a-1])\n",
    "    return Q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(100):\n",
    "Q_matrix = np.zeros((num_states, num_actions))\n",
    "input_df = pd.read_csv(infile)\n",
    "for row in input_df.iterrows():\n",
    "    s, a, r, sp = row[1][0], row[1][1], row[1][2], row[1][3] \n",
    "    Q_matrix = update_Q_matrix(Q_matrix, s, a, r, sp, gamma, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(outfile_name, \"w\")\n",
    "policy = np.argmax(Q_matrix, axis=1)\n",
    "for i in range(len(policy)):\n",
    "    #print(policy[i] +1)\n",
    "    outfile.write(str(policy[i]+1))\n",
    "    if i != (len(policy) - 1):\n",
    "        outfile.write('\\n')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_policy_file = \"policy_results_player_o_combined_100\"\n",
    "policy_df = pd.read_csv(cur_policy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2577\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "\n",
    "current_player = \"O\"\n",
    "opponent_player = \"X\"\n",
    "\n",
    "for num_sims in range(10000):\n",
    "    empty_board = np.array(['-']*9).reshape(3,3)\n",
    "    board = empty_board\n",
    "    \n",
    "    current_turn = \"X\"\n",
    "    x,y = np.where(board == '-')\n",
    "    possible_actions = [(x,y) for x,y in zip(x,y)]\n",
    "    avail_positions = [(x,y) for x,y in zip(x,y)]\n",
    "    #print(possible_actions)\n",
    "\n",
    "    empty_state_ind = all_states.index((0,0,0,0,0,0,0,0,0))\n",
    "    \n",
    "    while (not check_win(current_player, board) and not check_win(opponent_player, board) and (\"-\" in board)):\n",
    "        \n",
    "\n",
    "        # if it's our current player's turn (not opponent's turn)\n",
    "        # look at the policy for the move\n",
    "        if current_turn == current_player: \n",
    "            \n",
    "            # get the tuple of the current state\n",
    "            cur_state = [0,0,0,0,0,0,0,0,0]\n",
    "            cur_flat_board = board.flatten()\n",
    "            for i in range(len(cur_flat_board)):\n",
    "                if cur_flat_board[i] == \"X\":\n",
    "                    cur_state[i] = 1\n",
    "                elif cur_flat_board[i] == \"O\":\n",
    "                    cur_state[i] = 2\n",
    "            cur_state = tuple(cur_state)\n",
    "            \n",
    "            cur_state_ind = all_states.index(cur_state)\n",
    "            \n",
    "            chosen_move = policy_df['1'][cur_state_ind]\n",
    "            chosen_pos = possible_actions[chosen_move-1]\n",
    "            if chosen_pos in avail_positions:\n",
    "                board[chosen_pos[0]][chosen_pos[1]] = current_turn\n",
    "                avail_positions.remove(chosen_pos)\n",
    "            else:\n",
    "                rand_pos = random.choice(avail_positions)\n",
    "                board[rand_pos[0]][rand_pos[1]] = current_turn\n",
    "                avail_positions.remove(rand_pos)\n",
    "        else: # it's the opponent's turn (simulate by choosing randomly)\n",
    "            # choose random move\n",
    "            rand_pos = random.choice(avail_positions)\n",
    "            board[rand_pos[0]][rand_pos[1]] = current_turn\n",
    "            avail_positions.remove(rand_pos)\n",
    "\n",
    "        # switch to other player for next turn\n",
    "        if current_turn == \"X\":\n",
    "            current_turn = \"O\"\n",
    "        else:\n",
    "            current_turn = \"X\"\n",
    "            \n",
    "    # keep track of reward       \n",
    "    if check_win(current_player, board):\n",
    "        total_reward += 1\n",
    "    elif check_win(opponent_player, board):\n",
    "        total_reward += -1\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRESULTS (total rewards)\\n-----------------------\\n\\n(1 iterations of q-learning with player X with 300000 initial simulations)\\n\"policy_results_player_x_combined\": \\n\\n(100 iterations of q-learning with player X with 300000 initial simulations)\\n\"policy_results_player_x_combined_100\" : 3302 \\n\\n(1 iterations of q-learning with player O with 300000 initial simulations)\\n\"policy_results_player_x_combined\": \\n\\n(100 iterations of q-learning with player O with 300000 initial simulations)\\n\"policy_results_player_x_combined_100\" : 3302 \\n\\nrandom policy: \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "RESULTS (total rewards)\n",
    "-----------------------\n",
    "\n",
    "(1 iterations of q-learning with player X with 300000 initial simulations)\n",
    "\"policy_results_player_x_combined\": 3253\n",
    "\n",
    "(100 iterations of q-learning with player X with 300000 initial simulations)\n",
    "\"policy_results_player_x_combined_100\" : 3330 \n",
    "\n",
    "random policy player X: 3122\n",
    "\n",
    "(1 iterations of q-learning with player O with 300000 initial simulations)\n",
    "\"policy_results_player_x_combined\": -2670\n",
    "\n",
    "(100 iterations of q-learning with player O with 300000 initial simulations)\n",
    "\"policy_results_player_x_combined_100\" : -2571\n",
    "\n",
    "random policy player O: -2974\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    17960\n",
      "4      335\n",
      "2      237\n",
      "9      227\n",
      "8      222\n",
      "6      215\n",
      "7      165\n",
      "5      161\n",
      "3      160\n",
      "Name: 1, dtype: int64\n",
      "1    0.912509\n",
      "4    0.017021\n",
      "2    0.012041\n",
      "9    0.011533\n",
      "8    0.011279\n",
      "6    0.010924\n",
      "7    0.008383\n",
      "5    0.008180\n",
      "3    0.008129\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cur_policy_file = \"policy_results_player_x_combined\"\n",
    "policy_df = pd.read_csv(cur_policy_file)\n",
    "\n",
    "policy_counts_df = policy_df.loc[:,\"1\"].value_counts()\n",
    "print(policy_counts_df)\n",
    "policy_total = policy_df.sum()\n",
    "policy_total = 17960 + 335 + 237 + 227 + 222 + 215 + 165 + 161 + 160\n",
    "\n",
    "policy_counts_percentages_df = policy_counts_df.apply(lambda x: x/policy_total)\n",
    "print(policy_counts_percentages_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    17898\n",
      "4      444\n",
      "2      294\n",
      "6      243\n",
      "9      241\n",
      "8      230\n",
      "3      122\n",
      "7      105\n",
      "5      105\n",
      "Name: 1, dtype: int64\n",
      "1    0.909359\n",
      "4    0.022559\n",
      "2    0.014938\n",
      "6    0.012346\n",
      "9    0.012245\n",
      "8    0.011686\n",
      "3    0.006199\n",
      "7    0.005335\n",
      "5    0.005335\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cur_policy_file = \"policy_results_player_o_combined_100\"\n",
    "policy_df = pd.read_csv(cur_policy_file)\n",
    "\n",
    "policy_counts_df = policy_df.loc[:,\"1\"].value_counts()\n",
    "print(policy_counts_df)\n",
    "policy_total = policy_df.sum()\n",
    "policy_total = 17898 + 444 + 294 + 243 + 241 + 230 + 122 + 105 + 105\n",
    "\n",
    "policy_counts_percentages_df = policy_counts_df.apply(lambda x: x/policy_total)\n",
    "print(policy_counts_percentages_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
